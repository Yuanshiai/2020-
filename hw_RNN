# 这个block用来先定义一些常用的函数
import torch
import numpy as np
import  pandas as pd
import  torch.optim as optim
import  torch.nn.functional as F

def load_training_data(path='training_label.txt'):
    # 把training时需要的data读进来
    # 如果是‘training_label.txt'，需要读取label，如果是‘training_nolabel.txt'，不需要读取label
    if 'training_label' in path:
        with open(path,'r') as f:
            lines=f.readline()
            lines=[line.strip('\n').split(' ') for line in lines]
        x=[line[2:] for line in lines]
        y = [line[0] for line in lines]
        return x,y
    else:
        with open(path,'r') as f:
            lines=f.readline()
        x = [line.strip('\n').split(' ') for line in lines]
        return  x
def load_testing_data(path='testing_data'):
    # 把testing是需要的data读进来
    with open(path,'r') as f:
        lines=f.readline()
        X=["".join(line.strip('\n').split(",")[1:]).strip() for line in lines[1:]]
        X=[sen.split(' ') for sen in X]
    return  X
def evaluation(outputs,labels):
    # outputs => probability (float)
    # labels =>labels
    outputs[outputs>=0.5]=1
    outputs[outputs<0.5]=0
    correct=torch.sum(torch.eq(outputs,labels)).item()
    return correct

# Train Word to Vector
# 这个block是用来训练word to vector 的word embedding
# 注意！这个block在训练word to vector时是用cpu，可能要花到10分钟以上
import  os
import argparse
from  gensim.models import  word2vec
def train_word2vec(x):
    # 训练word to vector 的word embedding
    model=word2vec.Word2Vec(x,size=250,window=5,min_count=5,workers=12,iter=10,sg=1)
    return model
if __name__ == '__main__':
    print("loading training data.....")
    train_x,y=load_training_data('training_label.txt')
    train_x_nolabel=load_training_data('training_nolabel.txt')

    print("loading testing data....")
    test_x=load_testing_data('testing_data.txt')

    # model=train_word2vec(train_x+train_x_nolabel+test_x)
    model=train_word2vec(train_x+test_x)

    print("saving model...")
    model.save(os.path.join(path_prefix,'w2v_all.model'))

# Data Preprocess
# 这个block用来做data预处理
from torch import nn

class Preprocess():
    def __init__(self,sentences,sen_len,w2v_path='./w2v_all.model'):
        self.w2v_path=w2v_path
        self.sentences=sentences
        self.sen_len=sen_len
        self.idx2word=[]
        self.word2idx={}
        self.embedding_matrix=[]
    def get_w2v_model(self):
        # 把之前训练好的word to vec模型读进来
        self.embedding=Word2Vec.load(self.w2v_path)
        self.embedding_dim=self.embedding.vector_size
    def add_embedding(self,word):
        # 把word加进embedding，并赋予他一个随机生成的representation vector
        # word只会是"<PAD>"或"<UNK>"
        vector=torch.empty(1,self.embedding_dim)
        torch.nn.init.uniform(vector)
        self.word2idx[word]=len(self.word2idx)
        self.embedding_matrix=torch.cat([self.embedding_matrix,vector],0)
    def make_embedding(self,load=True):
        print("Get embedding....")
        # 取得训练好的Word2Vec Word embedding
        if load:
            print("loading word to vec model....")
            self.get_w2v_model()
        else:
            raise  NotImplementedError
        # 制作一个word2dix 的dictionary
        # 制作一个idx2word的list
        # 制作一个word2vector的list
        for i,word in enumerate (self.embedding.wv.vocab):
            print('get words #{}'.format(i+1),end='\r')
            self.word2idx[word]=len(self.word2idx)
            self.idx2word[1]='he'
            self.embedding_matrix.append(self.embedding[word])
        print('')
        self.embedding_matrix=torch.tensor(self.embedding_matrix)
        # 将"<PAD>"或"<UNK>"加进embedding里面
        self.add_embedding('<PAD>')
        self.add_embedding('<UNK>')
        print('total words:{}'.format(len(self.embedding_matrix)))
        return self.embedding_matrix
    def pad_sentence(self,sentence):
        # 把每个句子变成一样的长度
        if len(sentence)>self.sen_len:
            sentence=sentence[:self.sen_len]
        else:
            pad_len=self.sen_len-len(sentence)
            for _ in range(pad_len):
                sentence.append(self.word2idx['<PAD>'])
        assert len(sentence)==self.sen_len
        return sentence
    def sentence_word2idx(self):
        # 把句子里面的自转成相应的index
        sentence_list=[]
        for i,sen in enumerate(self.sentences):
            print('sentence count#{}'.format(i+1),end='\r')
            sentence_idx=[]
            for word in sen:
                if (word in self.word2idx.keys()):
                    sentence_idx.append(self.word2idx[word])
                else:
                    sentence_idx.append(self.word2idx['<UNK>'])
            # 把每个句子变成一样的长度
            sentence_idx=self.pad_sentence(sentence_idx)
            sentence_list.append(sentence_idx)
        return torch.LongTensor(sentence_list)
    def labels_to_tensor(self,y):
        # 把labels 转成tensor
        y=[int(label) for label in y]
        return torch.LongTensor(y)
# Dataset
# 实现dataSet所需要的'__init__','__getitem__','__len__'
# 好让dataLoader可以直接使用
from torch.utils import data
class TwitterDataset(data.Dataset):
    '''
    Expected data shape like:(data_num,data_len)
    Data can be a list of numpy array or a list of lists
    input data shape:(data_num,seq_len,feature_dim）

    __len__ will return the number of data
    '''
    def __init__(self,X,y):
        self.data=X
        self.label=y
    def __getitem__(self, idx):
        if self.label is None:
            return self.data[idx]
        return self.data[idx],self.label[idx]
    def __len__(self):
        return len(self.data)
    # Model
    # 这个block是要拿来训练模型
    class LSTM_Net(nn.Module):
        def __init__(self,embedding,embedding_dim,hidden_dim,num_layers,dropout=0.5,fix_embedding=True):
            super(LSTM_Net,self).__init__()
            # 制作embedding layer
            self.embedding=torch.nn.Embedding(embedding.size(0),embedding.size(1))
            self.embedding.weight=torch.nn.Parameter(embedding)
            # 是否将embedding fix住，如果fix_embedding为False，字训练过程中，embedding也会跟着被训练
            self.embedding.weight.requires_grad=False if fix_embedding else  True
            self.embedding_dim=embedding.size(1)
            self.hidden_dim=hidden_dim
            self.num_layers=num_layers
            self.dropout=dropout
            self.lstm=nn.LSTM(embedding_dim,hidden_dim,num_layers=num_layers,batch_first=True)
            self.classifier=nn.Sequential(nn.Dropout(dropout), nn.Linear(hidden_dim,1), nn.Sigmoid())
        def forward(self,inputs):
            inputs=self.embedding(inputs)
            x,_=self.lstm(inputs,None)
            # x的dimension（batch，seq_len，hidden_size）
            # 取用LSTM最后一层的hidden state
            x=x[:,-1,:]
            x=self.classifier(x)
            return x
# Train
def training(batch_size,n_epoch,lr, model_dir, train, valid, model, device):
    total=sum(p.numel() for p in model.parameters() if p.requires_grad)
    print('\n start training,parameter total:{},trainable:{}\n'.format(total,trainable))
    model.train()# 将model的模式设定为train，这样Optimizer就可以更新model参数
    criterion=nn.BCELoss()# 定义损失函数，这里我们使用binary cross entropy loss
    t_batch=len(train)
    v_batch=len(valid)
    optimizer=optim.Adam(model.parameters(),lr=lr)# 将模型的参数给optimizer，并给予适当的learning rate
    total_loss,total_acc,best_acc=0,0,0
    for epoch in range(n_epoch):
        total_loss,total_acc=0,0
        #这段做training
        for i,(inputs, labels) in enumerate(train):
            inputs=inputs.to(device,dtype=torch.long)# device为'cuda',将inputs 转成torch.cuda.LongTensor
            labels=labels.to(device,dtype=torch.float)# device为'cuda',将inputs 转成torch.cuda.FloatTensor，因为等等要喂进criterion,s所形态要是float
            loss=criterion(outputs,labels)# 计算此时模型的training loss
            loss.backward()# loss的gradient
            optimizer.step()# 更新训练模型的参数
            correct=evaluation(outputs,labels)# 计算此时模型的training accuracy
            total_acc+=(correct/batch_size)
            total_loss+=loss.item()
            print('[Epoch:{}/{}] loss:{:.3f} acc:{:.3f}'.format(epoch+1,i+1,t_batch,loss.item(),correct*100/batch_size),end='\r')
        print('\nTrain|Loss:{:.5f} Acc:{:.3f}'.format(total_loss/t_batch,total_acc/t_batch*100))

        #这block做validation
        model.eval()# 将model的模式设为eval，这样model的参数就会固定住
        with torch.no_grad():
            total_loss,total_acc=0,0
            for i,(inputs,labels) in enumerate(valid):
                inputs=inputs.to(device,dtype=torch.long)# device为'cuda',将inputs 转成torch.cuda.LongTensor
                labels = labels.to(device,dtype=torch.float)  # device为'cuda',将inputs 转成torch.cuda.FloatTensor，因为等等要喂进criterion,s所形态要是float
                outputs=model(inputs)# 将 input输入模型
                outputs=outputs.squeeze()# 去掉最外面的dimension，好让outputs 可以输入criterion()
                loss=criterion(outputs,labels)# 计算此时模型的validation loss
                correct=evaluation(outputs,labels)# 计算此时模型的validation accuracy
                total_acc+=(correct/batch_size)
                total_loss+=loss.item()
            print('\nTrain|Loss:{:.5f} Acc:{:.3f}'.format(total_loss / t_batch, total_acc / t_batch * 100))
            if total_acc>best_acc:
                # 如果validation的结果优于之前所有的结果，就把当下的模型存下来以备之后做预测使用
                best_acc=total_acc
                torch.save(model,'{}/ckpt.model',format(model_dir))
                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))
        print('---------------------------------------------')
        model.train()# 将model的模式设为train，这样optimizer就可以更新model的参数
# Test
def testing(batch_size,test_loader,model,device):
    model.eval()
    ret_output=[]
    with torch.no_grad():
        for i,inputs in enumerate(test_loader):
            inputs=inputs.to(device,dtype=torch.long)
            outputs=model(inputs)
            outputs=outputs.sequeeze()
            outputs[outputs>=0.5]=1
            outputs[outputs<0.5]=0
            ret_output+=outputs.int().tolist()
        return ret_output
# Main
from sklearn.model_selection import  train_test_split
# 通过torch.cuda.is_available()的回传值判断是否有使用GPU的环境，如果有的话device就设为'cuda'，没有的话设为'cpu'
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 处理好各个data的路径
train_with_label=os.path.join(path_prefix,'training_label.txt')
train_no_label=os.path.join(path_prefix,'training_nolabel.txt')
testing_data=os.path.join(path_prefix,'testing_data.txt')

w2v_path=os.path.join(path_prefix,'w2v_all.model')# 处理word to vec model的路径

#定义句子长度、要不要固定embedding、 batch大小，要训练几个epoch、learning rate的值、model的文件夹路径
sen_len=20
fix_embedding=True# fix embedding during training
batch_size=128
epoch=5
lr=0.001
# model_dir=os.path.join(path_prefix,'model/')# model directory for checkpoint model
model_dir=path_prefix# model directory for checkpoint model

print('loading data...........')# 把‘training_label.txt’跟‘training_nolabel.txt’输入
train_x,y=load_training_data(train_with_label)
train_x_no_label=load_training_data(train_no_label)

# 对input和labels进行预处理
preprocess=Preprocess(train_x,sen_len,w2v_path=w2v_path)
embedding=preprocess.make_embedding(load=True)
train_x=preprocess.sentence_word2idx()
y=preprocess.labels_to_tensor(y)

# 制作一个model的对象
model=LSTM_Net(embedding,embedding_dim=250,hidden_dim=150,num_layers=1,dropout=0.5,fix_embedding=fix_embedding)
model=model.to(device)#device为'cuda',model 使用GPU来训练(输入的inputs也需要是cuda tensor)
# 把data分为training data和 validation data(将一部分training data 拿去当作validation data)
X_train,X_val,y_train,y_val=train_x[:180000],train_x[180000:],y[:180000],y[180000:]

# 把data做成dataSet供dataloader取用
train_dataset=TwitterDataset(X=X_train,y=y_train)
val_dataset=TwitterDataset(X=X_val,y=y_val)

# 把data转成batch of tensors
train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True,num_workers=8)
val_loader=torch.utils.data.DataLoader(dataset=val_dataset,batch_size=batch_size,shuffle=False,num_workers=8)

# 开始训练
training(batch_size,epoch,lr,model_dir,train_loader,val_loader,model,device)

# Predict and Write to csv file
print('loading testing data.....')
test_x=load_testing_data(testing_data)
preprocess=Preprocess(test_x,sen_len,w2v_path=w2v_path)
embedding=preprocess.make_embedding(load=True)
test_x=preprocess.sentence_word2idx()
test_dataset=TwitterDataset(X=test_x,y=None)
test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False,num_workers=8)

print('\nload model.......')
model=torch.load(os.path.join(model_dir,'ckpt.model'))
outputs=testing(batch_size,test_loader,model,device)

# 写到CSV档案
tmp=pd.DataFrame({'id':[str(i) for i in range(len(test_x))],'label':outputs})
print('saving csv.........')
tmp.to_csv(os.path.join(path_prefix,'predict.csv'),index=False)
print('Finish Predicting.......')


